{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89739d1b-6681-4f86-bc86-e345ff134059",
   "metadata": {},
   "source": [
    "## Reflexion\n",
    "\n",
    "> **What you'll learn:** How to build a **Reflexion agent** that iteratively improves its responses through self-critique, external research, and structured revision — using LangGraph, Pydantic, and Tavily Search.\n",
    "\n",
    "The [Reflexion pattern](https://arxiv.org/pdf/2303.11366), introduced by Shinn et al., extends basic reflection by combining **self-critique** with **external knowledge integration** and **structured output parsing**. Unlike simple reflection, Reflexion allows an agent to learn from mistakes in real time while leveraging additional information.\n",
    "\n",
    "The workflow typically follows these steps:\n",
    "- **Initial Generation:** The agent produces a response along with self-critique and research queries.\n",
    "- **External Research:** Knowledge gaps identified during critique trigger web searches or other information retrieval.\n",
    "- **Knowledge Integration:** New insights are incorporated into an improved response.\n",
    "- **Iterative Refinement:** The agent repeats the cycle until the response meets desired quality thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5769d4-2822-41f1-8e76-28766cdeb360",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Import all required libraries: LangChain for LLM orchestration, LangGraph for workflow graphs, Pydantic for structured output validation, and Tavily for web search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd29b6ac-62e6-491d-8d63-b6b7d8681f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Import all dependencies for the Reflexion agent\n",
    "# Input   : None\n",
    "# Output  : Loaded modules in namespace\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "# --- LangChain & LangGraph ---\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import StructuredTool\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# --- Data validation & typing ---\n",
    "from pydantic import ValidationError, BaseModel, Field\n",
    "from typing import Literal, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# --- Utilities ---\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ab71bf-894f-4f6f-909f-8c4ea68ac33a",
   "metadata": {},
   "source": [
    "### Define LLM\n",
    "\n",
    "Initialize the language model that will power our Reflexion agent. The factory helpers abstract away API key loading and model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76242d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DUPLICATE — alternative LLM configuration, see cell below for active setup]\n",
    "# llm = ChatBedrock(\n",
    "#     model_id=\"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "#     region_name=\"us-west-2\",\n",
    "#     temperature=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2aeadaa-6b2b-46e8-8816-b13241c9ebb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM helpers imported successfully!\n",
      "LLM initialized: databricks-claude-opus-4-6 (Databricks)\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Import LLM helpers and initialize the language model\n",
    "# Input   : helpers/utils.py factory functions\n",
    "# Output  : `llm` — configured LLM instance\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "from helpers.utils import get_groq_llm, get_databricks_llm\n",
    "\n",
    "print(\"LLM helpers imported successfully!\")\n",
    "\n",
    "# --- Select LLM based on platform ---\n",
    "if sys.platform == \"win32\":\n",
    "    llm = get_groq_llm(\"openai/gpt-oss-120b\")\n",
    "elif sys.platform == \"darwin\":\n",
    "    llm = get_databricks_llm(\"databricks-claude-opus-4-6\")\n",
    "else:\n",
    "    print(\"linux\")\n",
    "\n",
    "# --- Confirm initialization ---\n",
    "if hasattr(llm, 'model_name'):\n",
    "    print(f\"LLM initialized: {llm.model_name}\")\n",
    "elif hasattr(llm, 'model'):\n",
    "    print(f\"LLM initialized: {llm.model} (Databricks)\")\n",
    "else:\n",
    "    print(\"LLM initialized: Groq LLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e082cd3-a0b0-43a5-96fe-b71908274fe8",
   "metadata": {},
   "source": [
    "### Actor Architecture\n",
    "\n",
    "At its core, a Reflexion agent is built around an **Actor** — an agent that generates an initial response, critiques it, and then re-executes the task with improvements. Supporting this loop are a few critical sub-components:\n",
    "\n",
    "- **Tool execution:** Access to external knowledge sources.\n",
    "- **Initial responder:** Generates the first draft along with self-reflection.\n",
    "- **Revisor/Revision:** Produces refined outputs by incorporating prior reflections.\n",
    "\n",
    "### Construct Tools\n",
    "\n",
    "Since Reflexion requires external knowledge, we first define a tool to fetch information from the web. Here we use `TavilySearchResults`, a wrapper around the Tavily Search API, enabling our agent to perform web searches and gather supporting evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ea06c45-4574-496b-baf7-c606ae509772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Initialize the Tavily web search tool\n",
    "# Input   : TAVILY_API_KEY (from environment)\n",
    "# Output  : `tavily_tool` — search tool returning up to 5 results\n",
    "# ─────────────────────────────────────────────\n",
    "# [DUPLICATE imports — TavilySearchResults, TavilySearchAPIWrapper already imported in cell above]\n",
    "\n",
    "web_search = TavilySearchAPIWrapper()\n",
    "tavily_tool = TavilySearchResults(api_wrapper=web_search, max_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d9ace",
   "metadata": {},
   "source": [
    "### Define the Prompt Template\n",
    "\n",
    "Next, let's define the prompt that will guide the actor agent's behavior. Prompts serve as the \"role description\" for an agent, specifying what it should and should not do. The agent is instructed to:\n",
    "\n",
    "- Provide an initial explanation.\n",
    "- Reflect and critique its own answer.\n",
    "- Generate search queries to fill knowledge gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a52c668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Define the actor's system prompt template\n",
    "# Input   : primary_instruction, function_name (partial vars)\n",
    "# Output  : `actor_prompt_template` — reusable prompt for both responder and revisor\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "actor_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert technical educator specializing in machine learning and neural networks.\n",
    "                Current time: {time}\n",
    "                1. {primary_instruction}\n",
    "                2. Reflect and critique your answer. Be severe to maximize improvement.\n",
    "                3. Recommend search queries to research information and improve your answer.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"\\n\\n<s>Reflect on the user's original question and the\"\n",
    "            \" actions taken thus far. Respond using the {function_name} function.</reminder>\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(\n",
    "    time=lambda: datetime.datetime.now().isoformat(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1198f2b9",
   "metadata": {},
   "source": [
    "### Enforce Structured Output\n",
    "\n",
    "When dealing with multi-step workflows, it's always recommended to define structured output models for each sub-agent. To ensure consistency, we define structured outputs using **Pydantic models**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "226cd8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Define Pydantic schemas for structured LLM output\n",
    "# Input   : None\n",
    "# Output  : `Reflection`, `GenerateResponse` — Pydantic models\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "class Reflection(BaseModel):\n",
    "    missing: str = Field(description=\"Critique of what is missing.\")\n",
    "    superfluous: str = Field(description=\"Critique of what is superfluous\")\n",
    "\n",
    "class GenerateResponse(BaseModel):\n",
    "    \"\"\"Generate response. Provide an answer, critique, and then follow up with search queries to improve the answer.\"\"\"\n",
    "    \n",
    "    response: str = Field(description=\"~250 word detailed answer to the question.\")\n",
    "    reflection: Reflection = Field(description=\"Your reflection on the initial answer.\")\n",
    "    research_queries: list[str] = Field(\n",
    "        description=\"1-3 search queries for researching improvements to address the critique of your current answer.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34775209",
   "metadata": {},
   "source": [
    "We use Pydantic's `BaseModel` to define two data classes:\n",
    "\n",
    "- **`Reflection`** captures the self-critique, requiring the agent to highlight what information is missing and what is superfluous (unnecessary).\n",
    "- **`GenerateResponse`** structures the final output. It ensures the agent provides its main response, includes a reflection (based on the `Reflection` class), and supplies a list of `research_queries`.\n",
    "\n",
    "This structured approach guarantees that our agents produce consistent and parseable responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad24906d",
   "metadata": {},
   "source": [
    "### Add Retry Logic\n",
    "\n",
    "Structured parsing can fail if the output doesn't match the schema. To address this, we add **retry logic with schema feedback** — when validation fails, the error and schema are fed back to the LLM for self-correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5b33bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Wrap an LLM chain with retry logic for structured output validation\n",
    "# Input   : chain (LLM pipeline), output_parser (Pydantic parser)\n",
    "# Output  : `AdaptiveResponder` class with `.generate()` method\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "class AdaptiveResponder:\n",
    "    def __init__(self, chain, output_parser):\n",
    "        self.chain = chain\n",
    "        self.output_parser = output_parser\n",
    "    \n",
    "    def generate(self, conversation_state: dict):\n",
    "        llm_response = None\n",
    "        for retry_count in range(3):\n",
    "            llm_response = self.chain.invoke(\n",
    "                {\"messages\": conversation_state[\"messages\"]}, {\"tags\": [f\"attempt:{retry_count}\"]}\n",
    "            )\n",
    "            try:\n",
    "                self.output_parser.invoke(llm_response)\n",
    "                return {\"messages\": llm_response}\n",
    "            except ValidationError as validation_error:\n",
    "                # Fix: Convert schema dict to JSON string\n",
    "                schema_json = json.dumps(self.output_parser.model_json_schema(), indent=2)\n",
    "                conversation_state = conversation_state + [\n",
    "                    llm_response,\n",
    "                    ToolMessage(\n",
    "                        content=f\"{repr(validation_error)}\\n\\nPay close attention to the function schema.\\n\\n{schema_json}\\n\\nRespond by fixing all validation errors.\",\n",
    "                        tool_call_id=llm_response.tool_calls[0][\"id\"],\n",
    "                    ),\n",
    "                ]\n",
    "        return {\"messages\": llm_response}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a999bbd8",
   "metadata": {},
   "source": [
    "### Bind the Data Model\n",
    "\n",
    "We now bind the `GenerateResponse` model as a tool. This forces the LLM to output exactly in the defined structure, and we wrap it with `AdaptiveResponder` for retry safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe76775a-ecc7-4c49-90f5-64be9380a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Build the initial responder chain with structured output\n",
    "# Input   : actor_prompt_template, llm, GenerateResponse schema\n",
    "# Output  : `initial_responder` — AdaptiveResponder for first-draft generation\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "initial_response_chain = actor_prompt_template.partial(\n",
    "    primary_instruction=\"Provide a detailed ~250 word explanation suitable for someone with basic programming background.\",\n",
    "    function_name=GenerateResponse.__name__,\n",
    ") | llm.bind_tools(tools=[GenerateResponse])\n",
    "\n",
    "response_parser = PydanticToolsParser(tools=[GenerateResponse])\n",
    "\n",
    "initial_responder = AdaptiveResponder(\n",
    "    chain=initial_response_chain, output_parser=response_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df2de7",
   "metadata": {},
   "source": [
    "After invoking `initial_response_chain`, we get a structured output that includes the initial answer, the self-critique, and the generated search queries. Let's test the initial responder with a simple query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82969ab9-ed5a-44b9-a273-552dfe71c247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'toolu_bdrk_017G1afAP24g9xiEtgs1qtrj', 'function': {'arguments': '{\"response\":\"**Supervised vs. Unsupervised Learning** are two fundamental paradigms in machine learning, and the key difference lies in whether your training data includes \\\\\"answers\\\\\" (labels) or not.\\\\n\\\\n**Supervised Learning** uses a dataset where each input has a corresponding correct output (label). Think of it like studying with an answer key. You feed the model examples like `(input, correct_answer)` pairs, and it learns to map inputs to outputs. For instance, given thousands of emails labeled \\\\\"spam\\\\\" or \\\\\"not spam,\\\\\" the model learns patterns to classify new, unseen emails. Common algorithms include Linear Regression, Decision Trees, and Neural Networks (for classification/regression). The goal is **prediction**.\\\\n\\\\n```python\\\\n# Supervised: you provide X (features) AND y (labels)\\\\nmodel.fit(X_train, y_train)\\\\nprediction = model.predict(X_new)\\\\n```\\\\n\\\\n**Unsupervised Learning** works with data that has **no labels**. The model must discover hidden structure or patterns on its own. Think of it like sorting a pile of objects without being told the categories — you\\'d naturally group similar items together. Common tasks include **clustering** (e.g., grouping customers by purchasing behavior) and **dimensionality reduction** (e.g., PCA for compressing features). Common algorithms include K-Means, DBSCAN, and Autoencoders.\\\\n\\\\n```python\\\\n# Unsupervised: you provide only X (no labels)\\\\nmodel.fit(X_train)\\\\nclusters = model.predict(X_new)\\\\n```\\\\n\\\\n**Key Differences Summary:**\\\\n| Aspect | Supervised | Unsupervised |\\\\n|---|---|---|\\\\n| Labels | Required | Not required |\\\\n| Goal | Predict outcomes | Discover patterns |\\\\n| Evaluation | Clear metrics (accuracy, MSE) | Harder to evaluate (silhouette score, etc.) |\\\\n| Examples | Spam detection, price prediction | Customer segmentation, anomaly detection |\\\\n\\\\nThere\\'s also **semi-supervised learning** (some labels) and **self-supervised learning** (generates its own labels), which blend these concepts.\",\"reflection\":{\"missing\":\"The answer could benefit from mentioning real-world use cases in more depth, discussing when to choose one over the other, and touching on the computational/data requirements differences. It also doesn\\'t mention the concept of reinforcement learning as a third major paradigm for completeness. The evaluation challenge of unsupervised learning deserves more emphasis — it\\'s a critical practical concern. Could also mention semi-supervised learning in more detail since it\\'s increasingly important in practice (e.g., modern LLMs use self-supervised pretraining).\",\"superfluous\":\"The code snippets, while helpful, are quite simplistic and might give the impression that the only API difference is whether you pass y or not, which oversimplifies things. The table at the end somewhat repeats information already stated in the prose. The mention of semi-supervised and self-supervised at the very end feels tacked on without enough context.\"},\"research_queries\":[\"supervised vs unsupervised learning practical use cases 2025\",\"when to use supervised vs unsupervised learning decision guide\",\"semi-supervised and self-supervised learning explained for beginners\"]}', 'name': 'GenerateResponse'}, 'type': 'function'}]}, response_metadata={'usage': {'prompt_tokens': 855, 'completion_tokens': 797, 'total_tokens': 1652}, 'prompt_tokens': 855, 'completion_tokens': 797, 'total_tokens': 1652, 'model': 'global.anthropic.claude-opus-4-6-v1', 'model_name': 'global.anthropic.claude-opus-4-6-v1', 'finish_reason': 'tool_calls'}, id='lc_run--019c9b6e-6e30-75b0-930b-4763e3ed2522-0', tool_calls=[{'name': 'GenerateResponse', 'args': {'response': '**Supervised vs. Unsupervised Learning** are two fundamental paradigms in machine learning, and the key difference lies in whether your training data includes \"answers\" (labels) or not.\\n\\n**Supervised Learning** uses a dataset where each input has a corresponding correct output (label). Think of it like studying with an answer key. You feed the model examples like `(input, correct_answer)` pairs, and it learns to map inputs to outputs. For instance, given thousands of emails labeled \"spam\" or \"not spam,\" the model learns patterns to classify new, unseen emails. Common algorithms include Linear Regression, Decision Trees, and Neural Networks (for classification/regression). The goal is **prediction**.\\n\\n```python\\n# Supervised: you provide X (features) AND y (labels)\\nmodel.fit(X_train, y_train)\\nprediction = model.predict(X_new)\\n```\\n\\n**Unsupervised Learning** works with data that has **no labels**. The model must discover hidden structure or patterns on its own. Think of it like sorting a pile of objects without being told the categories — you\\'d naturally group similar items together. Common tasks include **clustering** (e.g., grouping customers by purchasing behavior) and **dimensionality reduction** (e.g., PCA for compressing features). Common algorithms include K-Means, DBSCAN, and Autoencoders.\\n\\n```python\\n# Unsupervised: you provide only X (no labels)\\nmodel.fit(X_train)\\nclusters = model.predict(X_new)\\n```\\n\\n**Key Differences Summary:**\\n| Aspect | Supervised | Unsupervised |\\n|---|---|---|\\n| Labels | Required | Not required |\\n| Goal | Predict outcomes | Discover patterns |\\n| Evaluation | Clear metrics (accuracy, MSE) | Harder to evaluate (silhouette score, etc.) |\\n| Examples | Spam detection, price prediction | Customer segmentation, anomaly detection |\\n\\nThere\\'s also **semi-supervised learning** (some labels) and **self-supervised learning** (generates its own labels), which blend these concepts.', 'reflection': {'missing': \"The answer could benefit from mentioning real-world use cases in more depth, discussing when to choose one over the other, and touching on the computational/data requirements differences. It also doesn't mention the concept of reinforcement learning as a third major paradigm for completeness. The evaluation challenge of unsupervised learning deserves more emphasis — it's a critical practical concern. Could also mention semi-supervised learning in more detail since it's increasingly important in practice (e.g., modern LLMs use self-supervised pretraining).\", 'superfluous': 'The code snippets, while helpful, are quite simplistic and might give the impression that the only API difference is whether you pass y or not, which oversimplifies things. The table at the end somewhat repeats information already stated in the prose. The mention of semi-supervised and self-supervised at the very end feels tacked on without enough context.'}, 'research_queries': ['supervised vs unsupervised learning practical use cases 2025', 'when to use supervised vs unsupervised learning decision guide', 'semi-supervised and self-supervised learning explained for beginners']}, 'id': 'toolu_bdrk_017G1afAP24g9xiEtgs1qtrj', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 855, 'output_tokens': 797, 'total_tokens': 1652, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Test the initial responder with a sample question\n",
    "# Input   : example_question (str)\n",
    "# Output  : initial — dict with structured LLM response\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "example_question = \"What is the difference between supervised and unsupervised learning?\"\n",
    "initial = initial_responder.generate(\n",
    "    {\"messages\": [HumanMessage(content=example_question)]}\n",
    ")\n",
    "\n",
    "initial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530faa20-2947-4973-9097-236a3052a8ed",
   "metadata": {},
   "source": [
    "### Revision\n",
    "\n",
    "The **Revision** step represents the final stage of the Reflexion loop. Its purpose is to combine three critical elements — the original draft, the self-critique, and the research results — to produce a refined, evidence-backed response.\n",
    "\n",
    "We define a new instruction set (`improvement_guidelines`) that explicitly guides the Revisor:\n",
    "\n",
    "- Integrating critique into the revision process\n",
    "- Adding numerical citations tied to the research evidence\n",
    "- Differentiating correlation from causation in explanations\n",
    "- Including a structured **References** section with clean URLs only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62f8ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Define revision guidelines for the Revisor agent\n",
    "# Input   : None\n",
    "# Output  : `improvement_guidelines` — instruction string\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "improvement_guidelines = \"\"\"Revise your previous explanation using the new information.\n",
    "    - You should use the previous critique to add important technical details to your explanation.\n",
    "    - You MUST include numerical citations in your revised answer to ensure it can be verified.\n",
    "    - Add a \"References\" section to the bottom of your answer (which does not count towards the word limit).\n",
    "    - For the references field, provide a clean list of URLs only (e.g., [\"https://example.com\", \"https://example2.com\"])\n",
    "    - You should use the previous critique to remove superfluous information from your answer and make SURE it is not more than 250 words.\n",
    "    - Keep the explanation accessible for someone with basic programming background while being technically accurate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ddeca",
   "metadata": {},
   "source": [
    "To enforce output structure, we introduce `ImproveResponse` — a Pydantic schema that extends `GenerateResponse` with an additional `sources` field, ensuring that each improved answer comes with verifiable references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e43a8a5-8b84-49ed-80f3-a53d154f2c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Define the revision output schema with source citations\n",
    "# Input   : Inherits from GenerateResponse\n",
    "# Output  : `ImproveResponse` — Pydantic model with sources field\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "class ImproveResponse(GenerateResponse):\n",
    "    \"\"\"Improve your original answer to your question. Provide an answer, reflection,\n",
    "    cite your reflection with references, and finally\n",
    "    add search queries to improve the answer.\"\"\"\n",
    "    \n",
    "    sources: list[str] = Field(\n",
    "        description=\"List of reference URLs that support your answer. Each reference should be a clean URL string.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb7aa1f-f9e3-4e03-a784-2f66804c1973",
   "metadata": {},
   "source": [
    "With the schema defined, we construct the **revision chain** by binding the guidelines to the LLM and wrapping it with `AdaptiveResponder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0870d7b1-c476-441b-ba87-5ec7268d80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Build the revision chain with ImproveResponse schema\n",
    "# Input   : actor_prompt_template, llm, improvement_guidelines\n",
    "# Output  : `response_improver` — AdaptiveResponder for revised answers\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "improvement_chain = actor_prompt_template.partial(\n",
    "    primary_instruction=improvement_guidelines,\n",
    "    function_name=ImproveResponse.__name__,\n",
    ") | llm.bind_tools(tools=[ImproveResponse])\n",
    "\n",
    "improvement_parser = PydanticToolsParser(tools=[ImproveResponse])\n",
    "response_improver = AdaptiveResponder(chain=improvement_chain, output_parser=improvement_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811eaec0",
   "metadata": {},
   "source": [
    "We can now test the revision chain by providing a full conversation history — the initial draft, the critique, and the tool output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67f18a58-a625-4b22-ab7e-fcd56a2b9d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'toolu_bdrk_01GnzMaukjTHv6e8Xgt6s979', 'function': {'arguments': '{\"response\":\"**Supervised vs. Unsupervised Learning**\\\\n\\\\nThe core difference is whether your training data includes **labels** (known answers) or not.\\\\n\\\\n**Supervised Learning** uses labeled datasets — each input has a corresponding correct output. Think of it like studying with an answer key. The model learns to map inputs to outputs for **prediction** tasks [1]. Common applications include spam detection, image classification, medical diagnosis, and price forecasting [2]. Algorithms include Linear Regression, Decision Trees, and Neural Networks.\\\\n\\\\n```python\\\\n# Supervised: provide features AND labels\\\\nmodel.fit(X_train, y_train)\\\\nprediction = model.predict(X_new)\\\\n```\\\\n\\\\n**Unsupervised Learning** works with **unlabeled data**, discovering hidden structure on its own [1]. It\\'s ideal when you have large datasets but don\\'t know what outputs to expect [2]. Common tasks include **clustering** (grouping similar customers by purchasing behavior), **anomaly detection** (spotting fraudulent transactions), and **dimensionality reduction** (compressing features with PCA) [3]. Algorithms include K-Means, DBSCAN, and Autoencoders.\\\\n\\\\n```python\\\\n# Unsupervised: provide only features (no labels)\\\\nmodel.fit(X_train)\\\\nclusters = model.predict(X_new)\\\\n```\\\\n\\\\n**When to choose which?** Use supervised learning when you have labeled data and a clear prediction target. Use unsupervised learning when exploring unlabeled data for unknown patterns [4]. A critical practical difference: supervised models are easier to evaluate (accuracy, MSE), while unsupervised results are harder to validate since there\\'s no ground truth [3].\\\\n\\\\nIn practice, many systems combine both — for example, using unsupervised clustering to discover fraud patterns, then labeling those clusters to train a supervised classifier [5].\\\\n\\\\n**References**\\\\n[1] https://www.coursera.org/articles/supervised-vs-unsupervised-learning\\\\n[2] https://www.lightly.ai/blog/supervised-vs-unsupervised-learning\\\\n[3] https://www.linkedin.com/pulse/machine-learning-101-supervised-vs-unsupervised-ik3we\\\\n[4] https://biztechmagazine.com/article/2025/05/what-are-benefits-unsupervised-machine-learning-and-clustering-perfcon\\\\n[5] https://www.eleapsoftware.com/glossary/supervised-vs-unsupervised-learning-which-is-right-for-you/\",\"reflection\":{\"missing\":\"The answer could still benefit from mentioning semi-supervised and self-supervised learning as increasingly important paradigms (e.g., LLM pretraining uses self-supervised learning). It also doesn\\'t discuss the data cost implications — labeling data is expensive and time-consuming, which is a major practical reason unsupervised methods are attractive. The evaluation challenge for unsupervised learning could use specific metric examples (silhouette score, elbow method). Reinforcement learning as a third major paradigm is not mentioned for completeness.\",\"superfluous\":\"The code snippets, while illustrative, are very basic and may oversimplify the actual implementation differences. They take up word count that could be used for more substantive technical content. The answer is also slightly over the 250-word target with the code blocks included.\"},\"research_queries\":[\"self-supervised learning vs supervised unsupervised explained 2025\",\"cost of data labeling supervised learning challenges\",\"unsupervised learning evaluation metrics silhouette score explained\"],\"sources\":[\"https://www.coursera.org/articles/supervised-vs-unsupervised-learning\",\"https://www.lightly.ai/blog/supervised-vs-unsupervised-learning\",\"https://www.linkedin.com/pulse/machine-learning-101-supervised-vs-unsupervised-ik3we\",\"https://biztechmagazine.com/article/2025/05/what-are-benefits-unsupervised-machine-learning-and-clustering-perfcon\",\"https://www.eleapsoftware.com/glossary/supervised-vs-unsupervised-learning-which-is-right-for-you/\"]}', 'name': 'ImproveResponse'}, 'type': 'function'}]}, response_metadata={'usage': {'prompt_tokens': 4379, 'completion_tokens': 1057, 'total_tokens': 5436}, 'prompt_tokens': 4379, 'completion_tokens': 1057, 'total_tokens': 5436, 'model': 'global.anthropic.claude-opus-4-6-v1', 'model_name': 'global.anthropic.claude-opus-4-6-v1', 'finish_reason': 'tool_calls'}, id='lc_run--019c9b6f-403b-7c01-8ecf-e223b96d712c-0', tool_calls=[{'name': 'ImproveResponse', 'args': {'response': \"**Supervised vs. Unsupervised Learning**\\n\\nThe core difference is whether your training data includes **labels** (known answers) or not.\\n\\n**Supervised Learning** uses labeled datasets — each input has a corresponding correct output. Think of it like studying with an answer key. The model learns to map inputs to outputs for **prediction** tasks [1]. Common applications include spam detection, image classification, medical diagnosis, and price forecasting [2]. Algorithms include Linear Regression, Decision Trees, and Neural Networks.\\n\\n```python\\n# Supervised: provide features AND labels\\nmodel.fit(X_train, y_train)\\nprediction = model.predict(X_new)\\n```\\n\\n**Unsupervised Learning** works with **unlabeled data**, discovering hidden structure on its own [1]. It's ideal when you have large datasets but don't know what outputs to expect [2]. Common tasks include **clustering** (grouping similar customers by purchasing behavior), **anomaly detection** (spotting fraudulent transactions), and **dimensionality reduction** (compressing features with PCA) [3]. Algorithms include K-Means, DBSCAN, and Autoencoders.\\n\\n```python\\n# Unsupervised: provide only features (no labels)\\nmodel.fit(X_train)\\nclusters = model.predict(X_new)\\n```\\n\\n**When to choose which?** Use supervised learning when you have labeled data and a clear prediction target. Use unsupervised learning when exploring unlabeled data for unknown patterns [4]. A critical practical difference: supervised models are easier to evaluate (accuracy, MSE), while unsupervised results are harder to validate since there's no ground truth [3].\\n\\nIn practice, many systems combine both — for example, using unsupervised clustering to discover fraud patterns, then labeling those clusters to train a supervised classifier [5].\\n\\n**References**\\n[1] https://www.coursera.org/articles/supervised-vs-unsupervised-learning\\n[2] https://www.lightly.ai/blog/supervised-vs-unsupervised-learning\\n[3] https://www.linkedin.com/pulse/machine-learning-101-supervised-vs-unsupervised-ik3we\\n[4] https://biztechmagazine.com/article/2025/05/what-are-benefits-unsupervised-machine-learning-and-clustering-perfcon\\n[5] https://www.eleapsoftware.com/glossary/supervised-vs-unsupervised-learning-which-is-right-for-you/\", 'reflection': {'missing': \"The answer could still benefit from mentioning semi-supervised and self-supervised learning as increasingly important paradigms (e.g., LLM pretraining uses self-supervised learning). It also doesn't discuss the data cost implications — labeling data is expensive and time-consuming, which is a major practical reason unsupervised methods are attractive. The evaluation challenge for unsupervised learning could use specific metric examples (silhouette score, elbow method). Reinforcement learning as a third major paradigm is not mentioned for completeness.\", 'superfluous': 'The code snippets, while illustrative, are very basic and may oversimplify the actual implementation differences. They take up word count that could be used for more substantive technical content. The answer is also slightly over the 250-word target with the code blocks included.'}, 'research_queries': ['self-supervised learning vs supervised unsupervised explained 2025', 'cost of data labeling supervised learning challenges', 'unsupervised learning evaluation metrics silhouette score explained'], 'sources': ['https://www.coursera.org/articles/supervised-vs-unsupervised-learning', 'https://www.lightly.ai/blog/supervised-vs-unsupervised-learning', 'https://www.linkedin.com/pulse/machine-learning-101-supervised-vs-unsupervised-ik3we', 'https://biztechmagazine.com/article/2025/05/what-are-benefits-unsupervised-machine-learning-and-clustering-perfcon', 'https://www.eleapsoftware.com/glossary/supervised-vs-unsupervised-learning-which-is-right-for-you/']}, 'id': 'toolu_bdrk_01GnzMaukjTHv6e8Xgt6s979', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 4379, 'output_tokens': 1057, 'total_tokens': 5436, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Test the revision chain with initial response + search results\n",
    "# Input   : example_question, initial response, tavily_tool search output\n",
    "# Output  : revised — dict with improved, cited response\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "revised = response_improver.generate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=example_question),\n",
    "            initial[\"messages\"],\n",
    "            ToolMessage(\n",
    "                tool_call_id=initial[\"messages\"].tool_calls[0][\"id\"],\n",
    "                content=json.dumps(\n",
    "                    tavily_tool.invoke(\n",
    "                        {\n",
    "                            \"query\": initial[\"messages\"].tool_calls[0][\"args\"][\n",
    "                                \"research_queries\"\n",
    "                            ][0]\n",
    "                        }\n",
    "                    )\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "revised[\"messages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d90e2-3f37-41ab-ad82-50f53ae9f6d0",
   "metadata": {},
   "source": [
    "### Create Tool Node\n",
    "\n",
    "The next step is to execute the tool calls inside a LangGraph workflow. While the Responder and Revisor use different schemas, they both rely on the same external tool (a search API). The key differentiator of Reflexion is its ability to identify knowledge gaps and actively research solutions.\n",
    "\n",
    "The `ToolNode` automatically handles tool execution and result formatting, making it seamless to incorporate external knowledge sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46259dc4-bc28-4b33-9c23-19ffaa349cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Create search tool nodes for use inside the LangGraph workflow\n",
    "# Input   : tavily_tool, GenerateResponse/ImproveResponse schemas\n",
    "# Output  : `search_executor` — ToolNode that runs batch web searches\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "def execute_search_queries(research_queries: list[str], **kwargs):\n",
    "    \"\"\"Execute the generated search queries.\"\"\"\n",
    "    return tavily_tool.batch([{\"query\": search_term} for search_term in research_queries])\n",
    "\n",
    "# Tool node\n",
    "search_executor = ToolNode(\n",
    "    [\n",
    "        StructuredTool.from_function(execute_search_queries, name=GenerateResponse.__name__),\n",
    "        StructuredTool.from_function(execute_search_queries, name=ImproveResponse.__name__),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83a422d-7ff3-4999-a4b5-6689f7984dc4",
   "metadata": {},
   "source": [
    "### Building the Graph\n",
    "\n",
    "Finally, we assemble all components — **Responder**, **Tool Executor**, and **Revisor** — into a cyclical graph. This structure captures the iterative nature of Reflexion, where each loop strengthens the final answer.\n",
    "\n",
    "We define the graph state, loop control functions, and wire everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcf703dc-94a1-4510-bf33-aed57c32b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Define graph state, loop control, and assemble the Reflexion workflow\n",
    "# Input   : initial_responder, search_executor, response_improver\n",
    "# Output  : `reflexion_workflow` — compiled LangGraph state machine\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "# --- State schema ---\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- Loop control helpers ---\n",
    "def get_iteration_count(message_history: list):\n",
    "    \"\"\"Count recent tool/AI message cycles to track iteration depth.\"\"\"\n",
    "    iteration_count = 0\n",
    "    for message in message_history[::-1]:\n",
    "        if message.type not in {\"tool\", \"ai\"}:\n",
    "            break\n",
    "        iteration_count += 1\n",
    "    return iteration_count\n",
    "\n",
    "def determine_next_action(state: list):\n",
    "    \"\"\"Conditional edge: continue researching or stop after MAXIMUM_CYCLES.\"\"\"\n",
    "    current_iterations = get_iteration_count(state[\"messages\"])\n",
    "    if current_iterations > MAXIMUM_CYCLES:\n",
    "        return END\n",
    "    return \"search_and_research\"\n",
    "\n",
    "# --- Graph construction ---\n",
    "MAXIMUM_CYCLES = 5\n",
    "workflow_builder = StateGraph(State)\n",
    "\n",
    "workflow_builder.add_node(\"create_draft\", initial_responder.generate)\n",
    "workflow_builder.add_node(\"search_and_research\", search_executor)\n",
    "workflow_builder.add_node(\"enhance_response\", response_improver.generate)\n",
    "\n",
    "workflow_builder.add_edge(START, \"create_draft\")\n",
    "workflow_builder.add_edge(\"create_draft\", \"search_and_research\")\n",
    "workflow_builder.add_edge(\"search_and_research\", \"enhance_response\")\n",
    "workflow_builder.add_conditional_edges(\"enhance_response\", determine_next_action, [\"search_and_research\", END])\n",
    "\n",
    "reflexion_workflow = workflow_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1123c3da-2a8d-4e5c-9759-1605de84ecfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Install pygraphviz to draw graphs: `pip install pygraphviz`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ─────────────────────────────────────────────\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Purpose : Visualize the compiled Reflexion workflow graph\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Input   : reflexion_workflow\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Output  : Rendered graph diagram\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# ─────────────────────────────────────────────\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# [DUPLICATE import — Image, display already imported in cell above]\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m display(Image(\u001b[43mreflexion_workflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/AI_Engineering/Agentic_AI_Design_Pattern_Demystified/.venv/lib/python3.11/site-packages/langchain_core/runnables/graph.py:573\u001b[39m, in \u001b[36mGraph.draw_png\u001b[39m\u001b[34m(self, output_file_path, fontname, labels)\u001b[39m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_png\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PngDrawer  \u001b[38;5;66;03m# noqa: PLC0415\u001b[39;00m\n\u001b[32m    562\u001b[39m default_node_labels = {node.id: node.name \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nodes.values()}\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPngDrawer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfontname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m    \u001b[49m\u001b[43mLabelsDict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdefault_node_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnodes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m        \u001b[49m\u001b[43medges\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43medges\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codebases/AI_Engineering/Agentic_AI_Design_Pattern_Demystified/.venv/lib/python3.11/site-packages/langchain_core/runnables/graph_png.py:137\u001b[39m, in \u001b[36mPngDrawer.draw\u001b[39m\u001b[34m(self, graph, output_path)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _HAS_PYGRAPHVIZ:\n\u001b[32m    136\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInstall pygraphviz to draw graphs: `pip install pygraphviz`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# Create a directed graph\u001b[39;00m\n\u001b[32m    140\u001b[39m viz = pgv.AGraph(directed=\u001b[38;5;28;01mTrue\u001b[39;00m, nodesep=\u001b[32m0.9\u001b[39m, ranksep=\u001b[32m1.0\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: Install pygraphviz to draw graphs: `pip install pygraphviz`."
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Visualize the compiled Reflexion workflow graph\n",
    "# Input   : reflexion_workflow\n",
    "# Output  : Rendered graph diagram\n",
    "# ─────────────────────────────────────────────\n",
    "# [DUPLICATE import — Image, display already imported in cell above]\n",
    "\n",
    "display(Image(reflexion_workflow.get_graph().draw_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dfbba2-bb33-48e1-9d12-296d933101db",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Run the complete Reflexion agent end-to-end. The agent will iterate through draft → critique → research → revision cycles until the answer meets quality standards or reaches `MAXIMUM_CYCLES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed997d-a3df-43cc-b297-41bde131868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# Purpose : Run the Reflexion agent end-to-end and display each step\n",
    "# Input   : target_question (str), reflexion_workflow\n",
    "# Output  : Streamed step-by-step agent output\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "target_question = \"How do neural networks actually learn?\"\n",
    "\n",
    "print(f\"Running Reflexion agent with question: {target_question}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "events = reflexion_workflow.stream(\n",
    "    {\"messages\": [(\"user\", target_question)]},\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "\n",
    "for i, step in enumerate(events):\n",
    "    print(f\"\\nStep {i}\")\n",
    "    print(\"-\" * 40)\n",
    "    step[\"messages\"][-1].pretty_print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Reflexion agent execution completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd2936",
   "metadata": {},
   "source": [
    "### Key Takeaway\n",
    "\n",
    "The Reflexion agent demonstrates a powerful iterative improvement loop:\n",
    "\n",
    "1. **Generate** an initial technical explanation with self-critique\n",
    "2. **Identify** specific knowledge gaps requiring research\n",
    "3. **Execute** targeted web searches for current information\n",
    "4. **Integrate** findings into a comprehensive, cited response\n",
    "5. **Repeat** the process until the explanation meets quality standards\n",
    "\n",
    "By combining structured output (Pydantic), external tool use (Tavily), and cyclic graph execution (LangGraph), Reflexion goes beyond simple reflection — it actively learns and improves in real time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
